{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad7f8d92",
   "metadata": {},
   "source": [
    "# (6) Factor Augmented Vector Autoregressive (FAVAR) Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90733110",
   "metadata": {},
   "source": [
    "A factor augmented vector autoregressive (FAVAR) model with $p$ lags is defined by \n",
    "\n",
    "$$\n",
    "Y_{t} = c + \\Phi_{1}Y_{t-1} + ... + \\Phi_{p}Y_{t-p} + e_{t} \n",
    "$$\n",
    "\n",
    "where $Y_{t}$ is an $8 \\times 1$ vector of endogenous variables, $c$ defines a vector of constant terms, $\\Phi_{i}$ depicts our $8 \\times 8$ packed coefficient matricies for each $i$ lag to be determined during model estimation, and $e_{t}$ represents the vector of errors. The endogenous variables include one target series to be forecasted and seven mutually orthogonal principal components extracted from a large information set. Principal component analysis is a dimensionality reduction technique that is designed to return mutually orthogonal latent factors that maximize the variability within the original information set.\n",
    "\n",
    "Estimation is carried out by minimizing the forecast errors via an ordinary least squares (OLS)\n",
    "\n",
    "$$\n",
    "L(a_{1},...,a_{p}) = \\sum_{t}(Y_{t+1} - f_{t+1|t})^{2}.\n",
    "$$\n",
    "\n",
    "The optimal lag length of $p$ is set by minimizing the root mean squared error (RMSE) over the validation set. The following code reestimates the FAVAR model each period using walk foreword cross-validation on the validation set with a fixed lag length (p). Additionally, to avoid data leakage, principal component weights are updated each period after every observation becomes known to reflect the release of the most recent information. The FAVAR model is constructed with 8 predictors (one target and seven principal components). Therefore, the vectors $(Y_{t},c,e_{t})$ are $8 \\times 1$ and the matricies $\\Phi_{i}$ is $8 \\times 8$. Model validation is carried out using an 80-20 split. The initial training model is estimated on the first 80% of the training data. The training model weights are updated after each period. In other words, the training set expands with a window size of one-period. Therefore, model weights are always updated to reflect the most recent information. Walk foreword cross-validation carried out over the remaining 20% of the in-sample set. Each $h$-step ahead forecast is produced using linear model iteration. In the codes below, the phrase \"test\" actually references the “validation” set AND NOT an out-of-sample test set. \n",
    "\n",
    "The first block of code defines a function (MODEL) that takes in five arguments. The series to be forecasted is defined using the target argument. The information set is defined using feature_space. The p defines the number of autoregressive lags (AR_Lags) to set in the VAR model. The trend argument determines whether the model is estimated with a constant (c) or not (n) via the Const command. The number of multistep ahead forecasts are set using the step_size argument through horizons. The output of MODEL allows the researcher to analyze the number of observations in the training set (train_size), the training set predictions (train_pred), the test set predictions (test_pred), the training root mean squared error value (train_RMSE), the test set root mean squared error value (test_RMSE), the AIC, and BIC values. The evaluation metrics are stored in the Results DataFrame, which is used to determine the top-performing model via lag order selection. Therefore, the first block carries out our grid search methodology using walk-foreward cross-validation with the optimal lag length determined by validation set RMSE minimization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc0ef05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Library:\n",
    "from pandas import read_csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from statsmodels.tsa.api import VAR\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from matplotlib import pyplot\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# Function to Fit Model using Walk Foreward Cross-Validation:\n",
    "def MODEL(target, feature_space, p = 1, factors = 1, trend = 'n', step_size = 1):\n",
    "    # Extracting Data:\n",
    "    index_values = target.index.values\n",
    "    # Inital Training & Test Set Sizes:\n",
    "    train_size = int(len(target)*0.8)\n",
    "    test_size = len(target) - train_size\n",
    "    # Storage & Model Estimation:\n",
    "    test_pred = []\n",
    "    name = 'FAVAR('+str(p)+') Model'\n",
    "    print('-'*len(name))\n",
    "    print(name)\n",
    "    print('-'*len(name))\n",
    "    for t in range(test_size - step_size + 1):\n",
    "        # Tracking Convergence:\n",
    "        print('Test Set Walk Foreward: Iteration '+str(t+1))\n",
    "        # Define Walk Foreward Training Set:\n",
    "        target_train = target.values[:train_size+t]\n",
    "        feature_space_train = feature_space.values[:train_size+t, :]\n",
    "        # Define Walk Foreward Test Set:\n",
    "        target_test = target.values[train_size+t:]\n",
    "        feature_space_test = feature_space.values[train_size+t:, :]\n",
    "        # Define Normalization Functions:\n",
    "        feature_space_transform = StandardScaler().fit(feature_space_train)\n",
    "        # Fit Normalization to Training Sets:\n",
    "        feature_space_train = feature_space_transform.transform(feature_space_train)\n",
    "        # Apply Normalization to Test Set:\n",
    "        feature_space_test = feature_space_transform.transform(feature_space_test)\n",
    "        # Define Principal Component Analysis Functions:\n",
    "        feature_space_pca = PCA(n_components = factors, random_state = 1).fit(feature_space_train)\n",
    "        # Fit Principal Component Analysis to Training Set:\n",
    "        feature_space_train = feature_space_pca.transform(feature_space_train)\n",
    "        # Apply Principal Component Analysis to Test Set:\n",
    "        feature_space_test = feature_space_pca.transform(feature_space_test)\n",
    "        # Compile Data For FAVAR:\n",
    "        train_data = np.concatenate((target_train, feature_space_train), axis = 1)\n",
    "        test_data = np.concatenate((target_test, feature_space_test), axis = 1)\n",
    "        # Fit FAVAR(p) to Training Set:\n",
    "        model = VAR(train_data)\n",
    "        model_fit = model.fit(maxlags = p, trend = trend)\n",
    "        if t == 0:\n",
    "            train_pred = model_fit.fittedvalues[:,0]\n",
    "            AIC = model_fit.aic\n",
    "            BIC = model_fit.bic\n",
    "        # N-Step Ahead Forecast:\n",
    "        test_yhat = model_fit.forecast(y = train_data, steps = step_size)\n",
    "        test_pred = np.append(test_pred, test_yhat[step_size-1,0])\n",
    "    # Model Evaluation:\n",
    "    train_RMSE = np.sqrt(mean_squared_error(target.values[p:train_size], train_pred))\n",
    "    test_RMSE = np.sqrt(mean_squared_error(target.values[train_size+step_size-1:], test_pred))\n",
    "    return train_RMSE, test_RMSE, AIC, BIC\n",
    "# Setting Seed:\n",
    "np.random.seed(12345)\n",
    "# Load Data:\n",
    "All_Data = read_csv('Compiled_Data.csv', header = 0, index_col = 0, parse_dates = True)\n",
    "All_Data.index = pd.DatetimeIndex(All_Data.index.values, freq = \"MS\")\n",
    "# Seperate Target From Feature Block:\n",
    "housing_price = All_Data[['RHP']]\n",
    "All_Data = All_Data.drop('RHP', axis = 1)\n",
    "# Storage for Results & Hyperparameters:\n",
    "Results = pd.DataFrame(columns = ['FAVAR(p)', 'p', 'Factors', 'Train_RMSE', 'Test_RMSE', 'AIC', 'BIC'])\n",
    "# Setting Hyperparameters:\n",
    "AR_Lags = range(1,37,1)\n",
    "Factors = 7\n",
    "Const = 'c'\n",
    "horizons = 1\n",
    "for p in AR_Lags:\n",
    "    try:\n",
    "        train_RMSE, test_RMSE, AIC, BIC = MODEL(target = housing_price, feature_space = All_Data, p = p, factors = Factors, trend = Const, step_size = horizons)\n",
    "        model_performance = {'FAVAR(p)':'FAVAR('+str(p)+')', 'p':p, 'Factors':Factors, 'Train_RMSE':train_RMSE, 'Test_RMSE':test_RMSE, 'AIC':AIC, 'BIC':BIC}\n",
    "        Results = Results.append(model_performance, ignore_index = True)            \n",
    "    except:\n",
    "        continue "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0240d12c",
   "metadata": {},
   "source": [
    "The second block of code reestimates the top performing model after determining the optimal lag length (p)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ed41eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Library:\n",
    "from pandas import read_csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from statsmodels.tsa.api import VAR\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from matplotlib import pyplot\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# Function to Fit Model using Walk Foreward Cross-Validation:\n",
    "def MODEL(target, feature_space, p = 1, factors = 1, trend = 'n', step_size = 1):\n",
    "    # Extracting Data:\n",
    "    index_values = target.index.values\n",
    "    # Inital Training & Test Set Sizes:\n",
    "    train_size = int(len(target)*0.8)\n",
    "    test_size = len(target) - train_size\n",
    "    # Storage & Model Estimation:\n",
    "    test_pred = []\n",
    "    name = 'FAVAR('+str(p)+') Model'\n",
    "    print('-'*len(name))\n",
    "    print(name)\n",
    "    print('-'*len(name))\n",
    "    for t in range(test_size - step_size + 1):\n",
    "        # Tracking Convergence:\n",
    "        print('Test Set Walk Foreward: Iteration '+str(t+1))\n",
    "        # Define Walk Foreward Training Set:\n",
    "        target_train = target.values[:train_size+t]\n",
    "        feature_space_train = feature_space.values[:train_size+t, :]\n",
    "        # Define Walk Foreward Test Set:\n",
    "        target_test = target.values[train_size+t:]\n",
    "        feature_space_test = feature_space.values[train_size+t:, :]\n",
    "        # Define Normalization Functions:\n",
    "        feature_space_transform = StandardScaler().fit(feature_space_train)\n",
    "        # Fit Normalization to Training Sets:\n",
    "        feature_space_train = feature_space_transform.transform(feature_space_train)\n",
    "        # Apply Normalization to Test Set:\n",
    "        feature_space_test = feature_space_transform.transform(feature_space_test)\n",
    "        # Define Block Principal Component Analysis Functions:\n",
    "        feature_space_pca = PCA(n_components = factors, random_state = 1).fit(feature_space_train)\n",
    "        # Fit Principal Component Analysis to Training Set:\n",
    "        feature_space_train = feature_space_pca.transform(feature_space_train)\n",
    "        # Apply Principal Component Analysis to Test Set:\n",
    "        feature_space_test = feature_space_pca.transform(feature_space_test)\n",
    "        # Compile Data For FAVAR:\n",
    "        train_data = np.concatenate((target_train, feature_space_train), axis = 1)\n",
    "        test_data = np.concatenate((target_test, feature_space_test), axis = 1)\n",
    "        # Fit FAVAR(p) to Training Set:\n",
    "        model = VAR(train_data)\n",
    "        model_fit = model.fit(maxlags = p, trend = trend)\n",
    "        if t == 0:\n",
    "            train_pred = model_fit.fittedvalues[:,0]\n",
    "            AIC = model_fit.aic\n",
    "            BIC = model_fit.bic\n",
    "        # N-Step Ahead Forecast:\n",
    "        test_yhat = model_fit.forecast(y = train_data, steps = step_size)\n",
    "        test_pred = np.append(test_pred, test_yhat[step_size-1,0])\n",
    "    # Model Evaluation:\n",
    "    train_RMSE = np.sqrt(mean_squared_error(target.values[p:train_size], train_pred))\n",
    "    test_RMSE = np.sqrt(mean_squared_error(target.values[train_size+step_size-1:], test_pred))\n",
    "    # Convert Data to DataFrame:\n",
    "    train_pred = pd.DataFrame(train_pred, index = index_values[p:train_size], columns = ['train_pred'])\n",
    "    test_pred = pd.DataFrame(test_pred, index = index_values[train_size + step_size - 1:], columns = ['test_pred'])\n",
    "    return train_size, train_pred, test_pred, train_RMSE, test_RMSE, AIC, BIC\n",
    "# Setting Seed:\n",
    "np.random.seed(12345)\n",
    "# Load Data:\n",
    "All_Data = read_csv('Compiled_Data.csv', header = 0, index_col = 0, parse_dates = True)\n",
    "All_Data.index = pd.DatetimeIndex(All_Data.index.values, freq = \"MS\")\n",
    "# Seperate Target From Feature Block:\n",
    "housing_price = All_Data[['RHP']]\n",
    "All_Data = All_Data.drop('RHP', axis = 1)\n",
    "# Setting Hyperparameters:\n",
    "AR_Lags = Results.sort_values(by = 'Test_RMSE', ascending = True).iloc[0,1]\n",
    "Factors = Results.sort_values(by = 'Test_RMSE', ascending = True).iloc[0,2]\n",
    "Const = 'c'\n",
    "horizons = 1\n",
    "train_size, train_pred, test_pred, train_RMSE, test_RMSE, AIC, BIC = MODEL(target = housing_price, feature_space = All_Data, p = AR_Lags, factors = Factors, trend = Const, step_size = horizons)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc145f3",
   "metadata": {},
   "source": [
    "The third block presents and graphs the stored output from the MODEL function. The MODEL above is fit to housing price data in order to forecast real housing price growth rates at the U.S. national level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77409a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Autoregressive Moving Average Model: Growth Rate\n",
    "print('-----------------------------')\n",
    "print('National Housing Price Series')\n",
    "print('-----------------------------')\n",
    "print('Data Type: Growth Rates')\n",
    "print('Model Type: FAVAR('+str(AR_Lags)+')')\n",
    "print('Number of Factors Extracted: '+str(Factors))\n",
    "print('Train RMSE: %.3f' % (train_RMSE))\n",
    "print('Test RMSE: %.3f' % (test_RMSE))\n",
    "print('AIC: %.3f' % (AIC))\n",
    "print('BIC: %.3f' % (BIC))\n",
    "# Plot Forecast: Growth Rate\n",
    "sns.set_theme(style = 'whitegrid')\n",
    "pyplot.figure(figsize = (12,6))\n",
    "pyplot.plot(housing_price, label = 'Observed')\n",
    "pyplot.plot(train_pred, label = 'FAVAR('+str(AR_Lags)+': Train')\n",
    "pyplot.plot(test_pred, label = 'FAVAR('+str(AR_Lags)+'): Test')\n",
    "pyplot.xlabel('Date')\n",
    "pyplot.ylabel('Growth Rate')\n",
    "pyplot.title('Real Housing Price Series (National)')\n",
    "pyplot.legend()\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caafe0c1",
   "metadata": {},
   "source": [
    "The fourth block of code is used to analyze the forecast errors for stationarity. The forecast errors are computed, plotted, and distributed. Lastly, the autocorrelation function (ACF) is plotted and the Augmented Dickey-Fuller (ADF) unit root test is carried out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97da624",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Library:\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "# Define Residuals:\n",
    "resids = housing_price[AR_Lags:train_size] - train_pred.values\n",
    "# Plot Residuals:\n",
    "sns.set_theme(style = 'whitegrid')\n",
    "pyplot.figure(figsize = (16,4))\n",
    "pyplot.subplot(1,2,1)\n",
    "pyplot.plot(resids)\n",
    "pyplot.xlabel('Date')\n",
    "pyplot.title('Residual Series')\n",
    "pyplot.subplot(1,2,2)\n",
    "pyplot.hist(resids, bins = 20)\n",
    "pyplot.title('Residual Distribution')\n",
    "pyplot.tight_layout()\n",
    "pyplot.show()\n",
    "# Plot Autocorelation Function (ACF):\n",
    "sns.set_theme(style = 'whitegrid')\n",
    "fig, ax = pyplot.subplots(figsize=(8,4))\n",
    "plot_acf(resids, title = 'Residual ACF', lags = 36, ax = ax)\n",
    "pyplot.show()\n",
    "# ADF Test: Non-Stationary v. Stationary\n",
    "ADF_Test = adfuller(resids)\n",
    "print('----------------------')\n",
    "print('  ADF Unit-Root Test  ')\n",
    "print('----------------------')\n",
    "print('Test Statistic: %.3f' % (ADF_Test[0]))\n",
    "print('P-Value: %.3f' % (ADF_Test[1]))\n",
    "print('Critical Values:')\n",
    "for key, value in ADF_Test[4].items():\n",
    "    print('%s: %.3f' % (key, value))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be3f0f6d",
   "metadata": {},
   "source": [
    "The last block of code loads in the previous .csv files \"National_Train_Growth_One\" and \"National_Test_Growth_One\" that contain the stored forecasted values. The storage files are then augmented to include the predicted values from the current algorithm in order to estimate the forecast combinations, produce the final \"top performing\" model plots, and carry out the final comparison tests for predictive accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e0a6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Forecast Tables: \n",
    "train_forecasts = read_csv('National_Train_Growth_One.csv', header = 0, index_col = 0, parse_dates = True)\n",
    "train_forecasts.index = pd.DatetimeIndex(train_forecasts.index.values, freq = \"MS\")\n",
    "test_forecasts = read_csv('National_Test_Growth_One.csv', header = 0, index_col = 0, parse_dates = True)\n",
    "test_forecasts.index = pd.DatetimeIndex(test_forecasts.index.values, freq = \"MS\")\n",
    "# Add New Forecast Model:\n",
    "train_forecasts['FAVAR'] = train_pred\n",
    "test_forecasts['FAVAR'] = test_pred\n",
    "# Save Forecast:\n",
    "pd.DataFrame(train_forecasts).to_csv('National_Train_Growth_One.csv')\n",
    "pd.DataFrame(test_forecasts).to_csv('National_Test_Growth_One.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30da9fcf",
   "metadata": {},
   "source": [
    "# (16) Factor Augmented Vector Autoregressive Model with Elastic Net Estimation (FAVAR_Elastic_Net)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78dca210",
   "metadata": {},
   "source": [
    "A vector autoregressive (VAR) model with $p$ lags is defined by \n",
    "\n",
    "$$\n",
    "Y_{t} = c + \\sum_{i=1}^{p} \\Phi_{i}Y_{t-i} + e_{t}.\n",
    "$$\n",
    "\n",
    "where $Y_{t}$ is an $8 \\times 1$ vector of endogenous variables, $c$ is an $8 \\times 1 $ vector of equation constants, $\\Phi_{i}$ is an $8 \\times 8$ matrix of coefficients to be determined during model estimation, and $e_{t}$ is an $8 \\times 1$ vector of forecast errors. The vector of endogenous variables ($Y_{t}$) includes a target series to be forecasted and seven principal components that are extracted from the entire variable space. The resulting principal components are designed to be mutually orthogonal and maximize the variability within the original variable space. \n",
    "\n",
    "Elastic Net estimation is applied to every equation within the VAR framework. Elastic Net estimation is used to minimize forecast errors. Elastic Net estimation works by adding a penalty term designed to minimize the sum of squared coefficients and the sum of absolute coefficients. Therefore, the coefficients of less important predictors are pushed to zero and elastic net performs variables selection. Additionally, elastic net takes linear combinations of correlated predictors and works well in cases of multicolinearity.  \n",
    "\n",
    "$$\n",
    "L(a_{1},...,a_{n_{a}}) = \\sum_{t}(Y_{t+1} - Y_{t+1|t})^{2} + \\lambda_{1}\\sum_{j=1}^{n_{a}}|a_{j}| + \\lambda_{2}\\sum_{j=1}^{n_{a}}a_{j}^{2}\n",
    "$$\n",
    "\n",
    "The optimal lag length of $p$ is set to a length long enough to return white noise residuals. Reasonable penalty parameters ($\\lambda_{1},\\lambda_{2}$) are set using validation set root mean squared error (RMSE) minimization. The following code reestimates the VAR model each period using walk foreword cross-validation with a fixed lag length over the validation set. Model validation is carried out using an 80-20 split. The initial training model is estimated on the first 80% of the training data. The training model weights are updated after each peiord. Therefore, model weights are always updated to reflect the most recent information. Walk foreword cross-validation is carried out on the remaining 20% of the in-sample set. Each $h$-step ahead forecast is produced using linear model iteration. In the codes below, the phrase \"test\" actually references the “validation” set AND NOT an out-of-sample test set. \n",
    "\n",
    "In the Python Scikit-Learn library, the elastic net loss function is redefined to the following:\n",
    "\n",
    "$$\n",
    "L(a_{1},...,a_{n_{a}}) = \\sum_{t}(y_{t+1} - f_{t+1|t})^{2} + \\alpha \\lambda_{1}^{Ratio} \\sum_{j=1}^{n_{a}}|a_{j}| + \\alpha (1-\\lambda_{1}^{Ratio})\\sum_{j=1}^{n_{a}}a_{j}^{2}\n",
    "$$\n",
    "where $\\alpha = \\lambda_{1} + \\lambda_{2}$ and $\\lambda_{1}^{Ratio} = \\lambda_{1}/(\\lambda_{1} + \\lambda_{2})$. Here, $\\alpha$ is a homogenous hyperparameter that controls the strength of the penalty. Homogeneity implies that a doubling of $\\alpha$ imposes a doubling of each pentalty parameter, both equally and respectively. The Elastic Net Mixture is controlled by the hyperparameter $\\lambda_{1}^{Ratio}$. If $\\lambda_{1}^{Ratio} = 0$, then the Elastic Net loss function equals the Ridge Regression loss function. If $\\lambda_{1}^{Ratio} = 1$, then the Elastic Net loss function equals the Lasso Regression loss function. Therefore, our constraints are $\\alpha > 0$ and $0 < \\lambda_{1}^{Ratio} < 1$.\n",
    "\n",
    "The first block of code defines one function. The MODEL function takes in six arguments. The large information set is defined with the data argument. The target variable to be forecasted is defined with the target_name argument, which tells the algorithm what variable should be removed from the large information set to be forecasted from the resulting principal components. The number of lags to include in each equation is set using the lags argument. Lags should be set to a large enough number in order to return white noise residuals. The regularization parameters are set using the penalty and mixture arguments. Lastly, the number of forecast horizons is defined by step_size. The output of the MODEL function is designed to return the training and validation set RMSE values during regularization parameter grid searching. Additionally, the number of principal components is returned via factors. After a reasonable regularization parameter is set into the model, the MODEL function will then return the training and validation set predicted values. The first block of code defines a region to grid search in order to identify a reasonable regularization parameter. The second block of code sets the reasonable regularization parameter into the model and returns the forecasts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc8ee772",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Library:\n",
    "from pandas import read_csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from matplotlib import pyplot\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# Function to Fit Model using Walk Foreward Cross-Validation:\n",
    "def MODEL(data, target_name = 'RHP', lags = 36, penalty = 1.0, mixture = 0.5, step_size = 1):\n",
    "    # Solving the Hyperparameters:\n",
    "    lambda_1 = penalty*mixture\n",
    "    lambda_2 = penalty*(1-mixture)\n",
    "    # Seperate Target from Feature Space:\n",
    "    target = data[[target_name]].values\n",
    "    feature_space = data.drop(target_name, axis = 1).values\n",
    "    # Store Index Values:\n",
    "    index_values = data.index.values\n",
    "    # Store Number of Features and Extracted Factors:\n",
    "    features = feature_space.shape[1]\n",
    "    factors = 7 \n",
    "    # Store Training & Test Set Sizes:\n",
    "    train_size = int(data.shape[0]*0.8)\n",
    "    test_size = data.shape[0] - train_size\n",
    "    # Storage & Model Estimation:\n",
    "    test_pred = []\n",
    "    name = 'FAVAR-Type Elastic Net Regression'\n",
    "    print('-'*len(name))\n",
    "    print(name)\n",
    "    print('-'*len(name))\n",
    "    print('Alpha: ', penalty)\n",
    "    print('L1 Ratio: ', mixture)\n",
    "    for t in range(test_size - step_size + 1):\n",
    "        # Tracking Convergence:\n",
    "        print('Test Set Walk Foreward: Iteration '+str(t+1))\n",
    "        # Define Walk Foreward Training Sets:\n",
    "        target_train = target[:train_size+t]\n",
    "        feature_space_train = feature_space[:train_size+t, :]\n",
    "        # Define Walk Foreward Test Set:\n",
    "        target_test = target[train_size+t:]\n",
    "        feature_space_test = feature_space[train_size+t:, :]\n",
    "        # Define Normalization Functions:\n",
    "        feature_space_normalization = StandardScaler().fit(feature_space_train)\n",
    "        # Normalize Training Data:\n",
    "        feature_space_train = feature_space_normalization.transform(feature_space_train)\n",
    "        # Normalize Test Data:\n",
    "        feature_space_test = feature_space_normalization.transform(feature_space_test)\n",
    "        # Define Principal Component Analysis Function:\n",
    "        pca_function = PCA(n_components = factors, random_state = 1).fit(feature_space_train)\n",
    "        # Extract Training Set Factors:\n",
    "        feature_space_train = pca_function.transform(feature_space_train)\n",
    "        # Extract Test Set Factors:\n",
    "        feature_space_test = pca_function.transform(feature_space_test)\n",
    "        # Compile Data to Create Current & Lagged Data Sets:\n",
    "        Compiled_Features = np.concatenate((feature_space_train, feature_space_test), axis = 0)\n",
    "        Compiled_Target = np.concatenate((target_train, target_test), axis = 0)\n",
    "        Transformed_Data = np.concatenate((Compiled_Target, Compiled_Features), axis = 1)\n",
    "        Data_to_Use = Transformed_Data\n",
    "        for l in range(1,lags+1,1):\n",
    "            Lag_Transformed_Data = np.roll(Transformed_Data, l, axis = 0)\n",
    "            Data_to_Use = np.append(Data_to_Use, Lag_Transformed_Data, axis = 1)\n",
    "        Data_to_Use = Data_to_Use[lags:, :]\n",
    "        # Create Current & Lagged Data Sets:\n",
    "        Current_Data = Data_to_Use[:, 0:factors+1]\n",
    "        Lagged_Data = Data_to_Use[:, factors+1:]\n",
    "        # Redefine Walk Foreward Training Set Post Feature Extraction:\n",
    "        Lagged_Data_Train = Lagged_Data[:train_size-lags+t, :]\n",
    "        RHP_Train = Current_Data[:train_size-lags+t, 0]\n",
    "        Factor1_Train = Current_Data[:train_size-lags+t, 1]\n",
    "        Factor2_Train = Current_Data[:train_size-lags+t, 2]\n",
    "        Factor3_Train = Current_Data[:train_size-lags+t, 3]\n",
    "        Factor4_Train = Current_Data[:train_size-lags+t, 4]\n",
    "        Factor5_Train = Current_Data[:train_size-lags+t, 5]\n",
    "        Factor6_Train = Current_Data[:train_size-lags+t, 6]\n",
    "        Factor7_Train = Current_Data[:train_size-lags+t, 7]\n",
    "        # Redefine Walk Foreward Test Set Post Feature Extraction:\n",
    "        Lagged_Data_Test = Lagged_Data[train_size-lags+t:, :]\n",
    "        RHP_Test = Current_Data[train_size-lags+t:, 0]\n",
    "        Factor1_Test = Current_Data[train_size-lags+t:, 1]\n",
    "        Factor2_Test = Current_Data[train_size-lags+t:, 2]\n",
    "        Factor3_Test = Current_Data[train_size-lags+t:, 3]\n",
    "        Factor4_Test = Current_Data[train_size-lags+t:, 4]\n",
    "        Factor5_Test = Current_Data[train_size-lags+t:, 5]\n",
    "        Factor6_Test = Current_Data[train_size-lags+t:, 6]\n",
    "        Factor7_Test = Current_Data[train_size-lags+t:, 7]\n",
    "        # Fit Model to Training Set: RHP Equation\n",
    "        RHP_Model = ElasticNet(alpha = penalty, l1_ratio = mixture, random_state = 1)\n",
    "        RHP_Model.fit(X = Lagged_Data_Train, y = RHP_Train)\n",
    "        # Fit Model to Training Set: Factor1 Equation\n",
    "        Factor1_Model =  ElasticNet(alpha = penalty, l1_ratio = mixture, random_state = 1)\n",
    "        Factor1_Model.fit(X = Lagged_Data_Train, y = Factor1_Train)\n",
    "        # Fit Model to Training Set: Factor2 Equation\n",
    "        Factor2_Model =  ElasticNet(alpha = penalty, l1_ratio = mixture, random_state = 1)\n",
    "        Factor2_Model.fit(X = Lagged_Data_Train, y = Factor2_Train)\n",
    "        # Fit Model to Training Set: Factor3 Equation\n",
    "        Factor3_Model =  ElasticNet(alpha = penalty, l1_ratio = mixture, random_state = 1)\n",
    "        Factor3_Model.fit(X = Lagged_Data_Train, y = Factor3_Train)\n",
    "        # Fit Model to Training Set: Factor4 Equation\n",
    "        Factor4_Model =  ElasticNet(alpha = penalty, l1_ratio = mixture, random_state = 1)\n",
    "        Factor4_Model.fit(X = Lagged_Data_Train, y = Factor4_Train)\n",
    "        # Fit Model to Training Set: Factor5 Equation\n",
    "        Factor5_Model =  ElasticNet(alpha = penalty, l1_ratio = mixture, random_state = 1)\n",
    "        Factor5_Model.fit(X = Lagged_Data_Train, y = Factor5_Train)\n",
    "        # Fit Model to Training Set: Factor6 Equation\n",
    "        Factor6_Model =  ElasticNet(alpha = penalty, l1_ratio = mixture, random_state = 1)\n",
    "        Factor6_Model.fit(X = Lagged_Data_Train, y = Factor6_Train)\n",
    "        # Fit Model to Training Set: Factor7 Equation\n",
    "        Factor7_Model =  ElasticNet(alpha = penalty, l1_ratio = mixture, random_state = 1)\n",
    "        Factor7_Model.fit(X = Lagged_Data_Train, y = Factor7_Train)\n",
    "        # Forecast Storage:\n",
    "        forecast_storage = Lagged_Data[train_size-lags+t,:]\n",
    "        RHP_horizons = []\n",
    "        Factor1_horizons = []\n",
    "        Factor2_horizons = []\n",
    "        Factor3_horizons =[]\n",
    "        Factor4_horizons = []\n",
    "        Factor5_horizons = []\n",
    "        Factor6_horizons = []\n",
    "        Factor7_horizons = []\n",
    "        for h in range(step_size):\n",
    "            # Storing Iterative Forecasts:\n",
    "            RHP_horizons = np.append(RHP_horizons, RHP_Model.predict(X = forecast_storage[0:Lagged_Data.shape[1]].reshape(1,Lagged_Data.shape[1])))\n",
    "            Factor1_horizons = np.append(Factor1_horizons, Factor1_Model.predict(X = forecast_storage[0:Lagged_Data.shape[1]].reshape(1,Lagged_Data.shape[1])))\n",
    "            Factor2_horizons = np.append(Factor2_horizons, Factor2_Model.predict(X = forecast_storage[0:Lagged_Data.shape[1]].reshape(1,Lagged_Data.shape[1])))\n",
    "            Factor3_horizons = np.append(Factor3_horizons, Factor3_Model.predict(X = forecast_storage[0:Lagged_Data.shape[1]].reshape(1,Lagged_Data.shape[1])))\n",
    "            Factor4_horizons = np.append(Factor4_horizons, Factor4_Model.predict(X = forecast_storage[0:Lagged_Data.shape[1]].reshape(1,Lagged_Data.shape[1])))\n",
    "            Factor5_horizons = np.append(Factor5_horizons, Factor5_Model.predict(X = forecast_storage[0:Lagged_Data.shape[1]].reshape(1,Lagged_Data.shape[1])))\n",
    "            Factor6_horizons = np.append(Factor6_horizons, Factor6_Model.predict(X = forecast_storage[0:Lagged_Data.shape[1]].reshape(1,Lagged_Data.shape[1])))\n",
    "            Factor7_horizons = np.append(Factor7_horizons, Factor7_Model.predict(X = forecast_storage[0:Lagged_Data.shape[1]].reshape(1,Lagged_Data.shape[1])))\n",
    "            # Update Forecast Predictor Space:\n",
    "            forecast_storage = np.insert(forecast_storage, 0, RHP_horizons[h])\n",
    "            forecast_storage = np.insert(forecast_storage, 1, Factor1_horizons[h])\n",
    "            forecast_storage = np.insert(forecast_storage, 2, Factor2_horizons[h])\n",
    "            forecast_storage = np.insert(forecast_storage, 3, Factor3_horizons[h])\n",
    "            forecast_storage = np.insert(forecast_storage, 4, Factor4_horizons[h])\n",
    "            forecast_storage = np.insert(forecast_storage, 5, Factor5_horizons[h])\n",
    "            forecast_storage = np.insert(forecast_storage, 6, Factor6_horizons[h])\n",
    "            forecast_storage = np.insert(forecast_storage, 7, Factor7_horizons[h])\n",
    "        # Store Forecasted Values:\n",
    "        test_pred = np.append(test_pred, RHP_horizons[step_size - 1])\n",
    "        # Store Training Predictions:\n",
    "        if t == 0:\n",
    "            train_pred = RHP_Model.predict(X = Lagged_Data_Train)\n",
    "            train_RMSE = np.sqrt(mean_squared_error(RHP_Train, train_pred))\n",
    "    # Model Evaluation:\n",
    "    test_RMSE = np.sqrt(mean_squared_error(Current_Data[train_size-lags+step_size-1:, 0], test_pred))\n",
    "    return train_RMSE, test_RMSE, lambda_1, lambda_2, factors\n",
    "# Setting Seed:\n",
    "np.random.seed(12345)\n",
    "# Load Data:\n",
    "data = read_csv('Compiled_Data.csv', header = 0, index_col = 0, parse_dates = True)\n",
    "data.index = pd.DatetimeIndex(data.index.values, freq = \"MS\")\n",
    "# Set Model Hyperparameters:\n",
    "Target_Name = 'RHP'\n",
    "AR_Lags = 36\n",
    "L1_Ratio = np.arange(0.300,0.320,0.001)\n",
    "Alpha = np.arange(0.600,0.800,0.001)\n",
    "horizons = 1\n",
    "# Storage for Model Results:\n",
    "Results = pd.DataFrame(columns = ['Lags', 'Factors', 'Alpha', 'L1_Ratio', 'Lambda_1', 'Lambda_2', 'Train_RMSE', 'Test_RMSE'])\n",
    "for mixture in L1_Ratio:\n",
    "    for penalty in Alpha:\n",
    "        try:\n",
    "            train_RMSE, test_RMSE, lambda_1, lambda_2, factors = MODEL(data, target_name = Target_Name, lags = AR_Lags, penalty = penalty, mixture = mixture, step_size = horizons)\n",
    "            model_performance = {'Lags':AR_Lags, 'Factors':factors, 'Alpha':penalty, 'L1_Ratio':mixture, 'Lambda_1':lambda_1, 'Lambda_2':lambda_2, 'Train_RMSE':train_RMSE, 'Test_RMSE':test_RMSE}\n",
    "            Results = Results.append(model_performance, ignore_index = True)\n",
    "        except:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "992480cc",
   "metadata": {},
   "source": [
    "The second block of code reestimates the top performing model after setting the reasonable regularization parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d9cd6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Library:\n",
    "from pandas import read_csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from matplotlib import pyplot\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# Function to Fit Model using Walk Foreward Cross-Validation:\n",
    "def MODEL(data, target_name = 'RHP', lags = 36, penalty = 1.0, mixture = 0.5, step_size = 1):\n",
    "    # Solving the Hyperparameters:\n",
    "    lambda_1 = penalty*mixture\n",
    "    lambda_2 = penalty*(1-mixture)\n",
    "    # Seperate Target from Feature Space:\n",
    "    target = data[[target_name]].values\n",
    "    feature_space = data.drop(target_name, axis = 1).values\n",
    "    # Store Index Values:\n",
    "    index_values = data.index.values\n",
    "    # Store Number of Features and Extracted Factors:\n",
    "    features = feature_space.shape[1]\n",
    "    factors = 7 \n",
    "    # Store Training & Test Set Sizes:\n",
    "    train_size = int(data.shape[0]*0.8)\n",
    "    test_size = data.shape[0] - train_size\n",
    "    # Storage & Model Estimation:\n",
    "    test_pred = []\n",
    "    name = 'FAVAR-Type Elastic Net Regression'\n",
    "    print('-'*len(name))\n",
    "    print(name)\n",
    "    print('-'*len(name))\n",
    "    print('Alpha: ', penalty)\n",
    "    print('L1 Ratio: ', mixture)\n",
    "    for t in range(test_size - step_size + 1):\n",
    "        # Tracking Convergence:\n",
    "        print('Test Set Walk Foreward: Iteration '+str(t+1))\n",
    "        # Define Walk Foreward Training Sets:\n",
    "        target_train = target[:train_size+t]\n",
    "        feature_space_train = feature_space[:train_size+t, :]\n",
    "        # Define Walk Foreward Test Set:\n",
    "        target_test = target[train_size+t:]\n",
    "        feature_space_test = feature_space[train_size+t:, :]\n",
    "        # Define Normalization Functions:\n",
    "        feature_space_normalization = StandardScaler().fit(feature_space_train)\n",
    "        # Normalize Training Data:\n",
    "        feature_space_train = feature_space_normalization.transform(feature_space_train)\n",
    "        # Normalize Test Data:\n",
    "        feature_space_test = feature_space_normalization.transform(feature_space_test)\n",
    "        # Define Principal Component Analysis Function:\n",
    "        pca_function = PCA(n_components = factors, random_state = 1).fit(feature_space_train)\n",
    "        # Extract Training Set Factors:\n",
    "        feature_space_train = pca_function.transform(feature_space_train)\n",
    "        # Extract Test Set Factors:\n",
    "        feature_space_test = pca_function.transform(feature_space_test)\n",
    "        # Compile Data to Create Current & Lagged Data Sets:\n",
    "        Compiled_Features = np.concatenate((feature_space_train, feature_space_test), axis = 0)\n",
    "        Compiled_Target = np.concatenate((target_train, target_test), axis = 0)\n",
    "        Transformed_Data = np.concatenate((Compiled_Target, Compiled_Features), axis = 1)\n",
    "        Data_to_Use = Transformed_Data\n",
    "        for l in range(1,lags+1,1):\n",
    "            Lag_Transformed_Data = np.roll(Transformed_Data, l, axis = 0)\n",
    "            Data_to_Use = np.append(Data_to_Use, Lag_Transformed_Data, axis = 1)\n",
    "        Data_to_Use = Data_to_Use[lags:, :]\n",
    "        # Create Current & Lagged Data Sets:\n",
    "        Current_Data = Data_to_Use[:, 0:factors+1]\n",
    "        Lagged_Data = Data_to_Use[:, factors+1:]\n",
    "        # Redefine Walk Foreward Training Set Post Feature Extraction:\n",
    "        Lagged_Data_Train = Lagged_Data[:train_size-lags+t, :]\n",
    "        RHP_Train = Current_Data[:train_size-lags+t, 0]\n",
    "        Factor1_Train = Current_Data[:train_size-lags+t, 1]\n",
    "        Factor2_Train = Current_Data[:train_size-lags+t, 2]\n",
    "        Factor3_Train = Current_Data[:train_size-lags+t, 3]\n",
    "        Factor4_Train = Current_Data[:train_size-lags+t, 4]\n",
    "        Factor5_Train = Current_Data[:train_size-lags+t, 5]\n",
    "        Factor6_Train = Current_Data[:train_size-lags+t, 6]\n",
    "        Factor7_Train = Current_Data[:train_size-lags+t, 7]\n",
    "        # Redefine Walk Foreward Test Set Post Feature Extraction:\n",
    "        Lagged_Data_Test = Lagged_Data[train_size-lags+t:, :]\n",
    "        RHP_Test = Current_Data[train_size-lags+t:, 0]\n",
    "        Factor1_Test = Current_Data[train_size-lags+t:, 1]\n",
    "        Factor2_Test = Current_Data[train_size-lags+t:, 2]\n",
    "        Factor3_Test = Current_Data[train_size-lags+t:, 3]\n",
    "        Factor4_Test = Current_Data[train_size-lags+t:, 4]\n",
    "        Factor5_Test = Current_Data[train_size-lags+t:, 5]\n",
    "        Factor6_Test = Current_Data[train_size-lags+t:, 6]\n",
    "        Factor7_Test = Current_Data[train_size-lags+t:, 7]\n",
    "        # Fit Model to Training Set: RHP Equation\n",
    "        RHP_Model = ElasticNet(alpha = penalty, l1_ratio = mixture, random_state = 1)\n",
    "        RHP_Model.fit(X = Lagged_Data_Train, y = RHP_Train)\n",
    "        # Fit Model to Training Set: Factor1 Equation\n",
    "        Factor1_Model =  ElasticNet(alpha = penalty, l1_ratio = mixture, random_state = 1)\n",
    "        Factor1_Model.fit(X = Lagged_Data_Train, y = Factor1_Train)\n",
    "        # Fit Model to Training Set: Factor2 Equation\n",
    "        Factor2_Model =  ElasticNet(alpha = penalty, l1_ratio = mixture, random_state = 1)\n",
    "        Factor2_Model.fit(X = Lagged_Data_Train, y = Factor2_Train)\n",
    "        # Fit Model to Training Set: Factor3 Equation\n",
    "        Factor3_Model =  ElasticNet(alpha = penalty, l1_ratio = mixture, random_state = 1)\n",
    "        Factor3_Model.fit(X = Lagged_Data_Train, y = Factor3_Train)\n",
    "        # Fit Model to Training Set: Factor4 Equation\n",
    "        Factor4_Model =  ElasticNet(alpha = penalty, l1_ratio = mixture, random_state = 1)\n",
    "        Factor4_Model.fit(X = Lagged_Data_Train, y = Factor4_Train)\n",
    "        # Fit Model to Training Set: Factor5 Equation\n",
    "        Factor5_Model =  ElasticNet(alpha = penalty, l1_ratio = mixture, random_state = 1)\n",
    "        Factor5_Model.fit(X = Lagged_Data_Train, y = Factor5_Train)\n",
    "        # Fit Model to Training Set: Factor6 Equation\n",
    "        Factor6_Model =  ElasticNet(alpha = penalty, l1_ratio = mixture, random_state = 1)\n",
    "        Factor6_Model.fit(X = Lagged_Data_Train, y = Factor6_Train)\n",
    "        # Fit Model to Training Set: Factor7 Equation\n",
    "        Factor7_Model =  ElasticNet(alpha = penalty, l1_ratio = mixture, random_state = 1)\n",
    "        Factor7_Model.fit(X = Lagged_Data_Train, y = Factor7_Train)\n",
    "        # Forecast Storage:\n",
    "        forecast_storage = Lagged_Data[train_size-lags+t,:]\n",
    "        RHP_horizons = []\n",
    "        Factor1_horizons = []\n",
    "        Factor2_horizons = []\n",
    "        Factor3_horizons =[]\n",
    "        Factor4_horizons = []\n",
    "        Factor5_horizons = []\n",
    "        Factor6_horizons = []\n",
    "        Factor7_horizons = []\n",
    "        for h in range(step_size):\n",
    "            # Storing Iterative Forecasts:\n",
    "            RHP_horizons = np.append(RHP_horizons, RHP_Model.predict(X = forecast_storage[0:Lagged_Data.shape[1]].reshape(1,Lagged_Data.shape[1])))\n",
    "            Factor1_horizons = np.append(Factor1_horizons, Factor1_Model.predict(X = forecast_storage[0:Lagged_Data.shape[1]].reshape(1,Lagged_Data.shape[1])))\n",
    "            Factor2_horizons = np.append(Factor2_horizons, Factor2_Model.predict(X = forecast_storage[0:Lagged_Data.shape[1]].reshape(1,Lagged_Data.shape[1])))\n",
    "            Factor3_horizons = np.append(Factor3_horizons, Factor3_Model.predict(X = forecast_storage[0:Lagged_Data.shape[1]].reshape(1,Lagged_Data.shape[1])))\n",
    "            Factor4_horizons = np.append(Factor4_horizons, Factor4_Model.predict(X = forecast_storage[0:Lagged_Data.shape[1]].reshape(1,Lagged_Data.shape[1])))\n",
    "            Factor5_horizons = np.append(Factor5_horizons, Factor5_Model.predict(X = forecast_storage[0:Lagged_Data.shape[1]].reshape(1,Lagged_Data.shape[1])))\n",
    "            Factor6_horizons = np.append(Factor6_horizons, Factor6_Model.predict(X = forecast_storage[0:Lagged_Data.shape[1]].reshape(1,Lagged_Data.shape[1])))\n",
    "            Factor7_horizons = np.append(Factor7_horizons, Factor7_Model.predict(X = forecast_storage[0:Lagged_Data.shape[1]].reshape(1,Lagged_Data.shape[1])))\n",
    "            # Update Forecast Predictor Space:\n",
    "            forecast_storage = np.insert(forecast_storage, 0, RHP_horizons[h])\n",
    "            forecast_storage = np.insert(forecast_storage, 1, Factor1_horizons[h])\n",
    "            forecast_storage = np.insert(forecast_storage, 2, Factor2_horizons[h])\n",
    "            forecast_storage = np.insert(forecast_storage, 3, Factor3_horizons[h])\n",
    "            forecast_storage = np.insert(forecast_storage, 4, Factor4_horizons[h])\n",
    "            forecast_storage = np.insert(forecast_storage, 5, Factor5_horizons[h])\n",
    "            forecast_storage = np.insert(forecast_storage, 6, Factor6_horizons[h])\n",
    "            forecast_storage = np.insert(forecast_storage, 7, Factor7_horizons[h])\n",
    "        # Store Forecasted Values:\n",
    "        test_pred = np.append(test_pred, RHP_horizons[step_size - 1])\n",
    "        # Store Training Predictions:\n",
    "        if t == 0:\n",
    "            train_pred = RHP_Model.predict(X = Lagged_Data_Train)\n",
    "            train_RMSE = np.sqrt(mean_squared_error(RHP_Train, train_pred))\n",
    "    # Model Evaluation:\n",
    "    test_RMSE = np.sqrt(mean_squared_error(Current_Data[train_size-lags+step_size-1:, 0], test_pred))\n",
    "    train_pred = pd.DataFrame(train_pred, index = index_values[lags:train_size], columns = ['train_pred'])\n",
    "    test_pred = pd.DataFrame(test_pred, index = index_values[data.shape[0]-test_size+step_size-1:], columns = ['test_pred'])\n",
    "    return train_RMSE, test_RMSE, train_pred, test_pred, lambda_1, lambda_2\n",
    "# Setting Seed:\n",
    "np.random.seed(12345)\n",
    "# Load Data:\n",
    "data = read_csv('Compiled_Data.csv', header = 0, index_col = 0, parse_dates = True)\n",
    "data.index = pd.DatetimeIndex(data.index.values, freq = \"MS\")\n",
    "# Set Model Hyperparameters:\n",
    "Target_Name = 'RHP'\n",
    "target_series = data[[Target_Name]]\n",
    "lags = Results.sort_values(by = 'Test_RMSE', ascending = True).iloc[0,0]\n",
    "penalty = Results.sort_values(by = 'Test_RMSE', ascending = True).iloc[0,2]\n",
    "mixture = Results.sort_values(by = 'Test_RMSE', ascending = True).iloc[0,3]\n",
    "horizons = 1\n",
    "# Evaluate Model:\n",
    "train_RMSE, test_RMSE, train_pred, test_pred, lambda_1, lambda_2 = MODEL(data, target_name = Target_Name, lags = lags, penalty = penalty, mixture = mixture, step_size = horizons)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c2f376f",
   "metadata": {},
   "source": [
    "The third block presents and graphs the stored output from the MODEL function. The MODEL above is fit to housing price data in order to forecast real housing price growth rates at the U.S. national level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f20cf516",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Model: Growth Rates\n",
    "print('-----------------------------')\n",
    "print('National Housing Price Series')\n",
    "print('-----------------------------')\n",
    "print('Data Type: Growth Rates')\n",
    "print('Model Type: FAVAR-Type Elastic Net Regression')\n",
    "print('Alpha (Strength) Hyperparameter: ', penalty)\n",
    "print('L1 Ratio (Mixture) Hyperparameter: ', mixture)\n",
    "print('Lambda 1 (L1) Hyperparameter: ', lambda_1)\n",
    "print('Lambda 2 (L2) Hyperparameter: ', lambda_2)\n",
    "print('Train RMSE: %.3f' % (train_RMSE))\n",
    "print('Test RMSE: %.3f' % (test_RMSE))\n",
    "# Plot Forecast: Growth Rates\n",
    "sns.set_theme(style = 'whitegrid')\n",
    "pyplot.figure(figsize = (12,6))\n",
    "pyplot.plot(target_series, label = 'Observed')\n",
    "pyplot.plot(train_pred, label = 'FAVAR_Elastic_Net: Train')\n",
    "pyplot.plot(test_pred, label = 'FAVAR_Elastic_Net: Test')\n",
    "pyplot.xlabel('Date')\n",
    "pyplot.ylabel('Growth Rate')\n",
    "pyplot.title('Real Housing Price Series (National)')\n",
    "pyplot.legend()\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29850b22",
   "metadata": {},
   "source": [
    "The fourth block of code is used to analyze the forecast errors for stationarity. The forecast errors are computed, plotted, and distributed. Lastly, the autocorrelation function (ACF) is plotted and the Augmented Dickey-Fuller (ADF) unit root test is carried out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e063d28f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Library:\n",
    "import pandas as pd\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "# Compute Model Residuals:\n",
    "Error = pd.concat([target_series,train_pred], axis = 1)\n",
    "Error = Error.dropna()\n",
    "Error['Resids'] = Error.iloc[:,0] - Error.iloc[:,1]\n",
    "# Plot Residuals:\n",
    "sns.set_theme(style = 'whitegrid')\n",
    "pyplot.figure(figsize = (16,4))\n",
    "pyplot.subplot(1,2,1)\n",
    "pyplot.plot(Error['Resids'])\n",
    "pyplot.xlabel('Date')\n",
    "pyplot.title('Residual Series')\n",
    "pyplot.subplot(1,2,2)\n",
    "pyplot.hist(Error['Resids'], bins = 20)\n",
    "pyplot.title('Residual Distribution')\n",
    "pyplot.tight_layout()\n",
    "pyplot.show()\n",
    "# Plot Autocorelation Function (ACF):\n",
    "sns.set_theme(style = 'whitegrid')\n",
    "fig, ax = pyplot.subplots(figsize=(8,4))\n",
    "plot_acf(Error['Resids'], title = 'Residual ACF', lags = 36, ax = ax)\n",
    "pyplot.show()\n",
    "# ADF Test: Non-Stationary v. Stationary\n",
    "ADF_Test = adfuller(Error['Resids'])\n",
    "print('----------------------')\n",
    "print('  ADF Unit-Root Test  ')\n",
    "print('----------------------')\n",
    "print('Test Statistic: %.3f' % (ADF_Test[0]))\n",
    "print('P-Value: %.3f' % (ADF_Test[1]))\n",
    "print('Critical Values:')\n",
    "for key, value in ADF_Test[4].items():\n",
    "    print('%s: %.3f' % (key, value))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb1bd51",
   "metadata": {},
   "source": [
    "The last block of code loads in the previous .csv files \"National_Train_Growth_One\" and \"National_Test_Growth_One\" that contain the stored forecasted values. The storage files are then augmented to include the predicted values from the current algorithm in order to estimate the forecast combinations, produce the final \"top performing\" model plots, and carry out the final comparison tests for predictive accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df40f321",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Forecast Tables: \n",
    "train_forecasts = read_csv('National_Train_Growth_One.csv', header = 0, index_col = 0, parse_dates = True)\n",
    "train_forecasts.index = pd.DatetimeIndex(train_forecasts.index.values, freq = \"MS\")\n",
    "test_forecasts = read_csv('National_Test_Growth_One.csv', header = 0, index_col = 0, parse_dates = True)\n",
    "test_forecasts.index = pd.DatetimeIndex(test_forecasts.index.values, freq = \"MS\")\n",
    "# Add New Forecast Model:\n",
    "train_forecasts['FAVAR_Elastic_Net'] = train_pred\n",
    "test_forecasts['FAVAR_Elastic_Net'] = test_pred\n",
    "# Save Forecast:\n",
    "pd.DataFrame(train_forecasts).to_csv('National_Train_Growth_One.csv')\n",
    "pd.DataFrame(test_forecasts).to_csv('National_Test_Growth_One.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
